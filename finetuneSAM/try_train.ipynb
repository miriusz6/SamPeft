{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HAWkO7kwZUSQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#import os\n",
        "#import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "from models.sam import SamAutomaticMaskGenerator, SamPredictor, sam_model_registry\n",
        "# from PIL import ImageDraw\n",
        "# from mobile_sam.utils.tools import box_prompt, format_results, point_prompt\n",
        "#from app.utils.tools_gradio import fast_process\n",
        "# Most of our demo code is from [FastSAM Demo](https://huggingface.co/spaces/An-619/FastSAM). Huge thanks for AN-619.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cfg import parse_args\n",
        "args =  parse_args()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\build_sam.py:157: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sam(\n",
              "  (image_encoder): TinyViT(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (seq): Sequential(\n",
              "        (0): Conv2d_BN(\n",
              "          (c): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Conv2d_BN(\n",
              "          (c): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): ConvLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x MBConv(\n",
              "            (conv1): Conv2d_BN(\n",
              "              (c): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d_BN(\n",
              "              (c): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act2): GELU(approximate='none')\n",
              "            (conv3): Conv2d_BN(\n",
              "              (c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act3): GELU(approximate='none')\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (downsample): PatchMerging(\n",
              "          (act): GELU(approximate='none')\n",
              "          (conv1): Conv2d_BN(\n",
              "            (c): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv2): Conv2d_BN(\n",
              "            (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv3): Conv2d_BN(\n",
              "            (c): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): BasicLayer(\n",
              "        dim=128, input_resolution=(128, 128), depth=2\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x TinyViTBlock(\n",
              "            dim=128, input_resolution=(128, 128), num_heads=4, window_size=7, mlp_ratio=4.0\n",
              "            (drop_path): Identity()\n",
              "            (attn): Attention(\n",
              "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (mlp): Mlp(\n",
              "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (local_conv): Conv2d_BN(\n",
              "              (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (downsample): PatchMerging(\n",
              "          (act): GELU(approximate='none')\n",
              "          (conv1): Conv2d_BN(\n",
              "            (c): Conv2d(128, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv2): Conv2d_BN(\n",
              "            (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)\n",
              "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv3): Conv2d_BN(\n",
              "            (c): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): BasicLayer(\n",
              "        dim=160, input_resolution=(64, 64), depth=6\n",
              "        (blocks): ModuleList(\n",
              "          (0-5): 6 x TinyViTBlock(\n",
              "            dim=160, input_resolution=(64, 64), num_heads=5, window_size=14, mlp_ratio=4.0\n",
              "            (drop_path): Identity()\n",
              "            (attn): Attention(\n",
              "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
              "              (proj): Linear(in_features=160, out_features=160, bias=True)\n",
              "            )\n",
              "            (mlp): Mlp(\n",
              "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
              "              (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (local_conv): Conv2d_BN(\n",
              "              (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
              "              (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (downsample): PatchMerging(\n",
              "          (act): GELU(approximate='none')\n",
              "          (conv1): Conv2d_BN(\n",
              "            (c): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv2): Conv2d_BN(\n",
              "            (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
              "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv3): Conv2d_BN(\n",
              "            (c): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): BasicLayer(\n",
              "        dim=320, input_resolution=(64, 64), depth=2\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x TinyViTBlock(\n",
              "            dim=320, input_resolution=(64, 64), num_heads=10, window_size=7, mlp_ratio=4.0\n",
              "            (drop_path): Identity()\n",
              "            (attn): Attention(\n",
              "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
              "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "            )\n",
              "            (mlp): Mlp(\n",
              "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
              "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (local_conv): Conv2d_BN(\n",
              "              (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
              "              (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm_head): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "    (head): Linear(in_features=320, out_features=1000, bias=True)\n",
              "    (neck): Sequential(\n",
              "      (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): LayerNorm2d()\n",
              "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (3): LayerNorm2d()\n",
              "    )\n",
              "  )\n",
              "  (prompt_encoder): PromptEncoder(\n",
              "    (pe_layer): PositionEmbeddingRandom()\n",
              "    (point_embeddings): ModuleList(\n",
              "      (0-3): 4 x Embedding(1, 256)\n",
              "    )\n",
              "    (not_a_point_embed): Embedding(1, 256)\n",
              "    (mask_downscaling): Sequential(\n",
              "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): LayerNorm2d()\n",
              "      (5): GELU(approximate='none')\n",
              "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (no_mask_embed): Embedding(1, 256)\n",
              "  )\n",
              "  (mask_decoder): MaskDecoder(\n",
              "    (transformer): TwoWayTransformer(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x TwoWayAttentionBlock(\n",
              "          (self_attn): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_token_to_image): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (act): ReLU()\n",
              "          )\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_image_to_token): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_attn_token_to_image): Attention(\n",
              "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (iou_token): Embedding(1, 256)\n",
              "    (mask_tokens): Embedding(3, 256)\n",
              "    (output_upscaling): Sequential(\n",
              "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): GELU(approximate='none')\n",
              "    )\n",
              "    (output_hypernetworks_mlps): ModuleList(\n",
              "      (0-2): 3 x MLP(\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (iou_prediction_head): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=3, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from cfg import parse_args\n",
        "\n",
        "# Load the pre-trained model\n",
        "sam_checkpoint = \"mobile_sam.pt\"\n",
        "# Define the model type: Tiny Vit\n",
        "model_type = \"vit_t\"\n",
        "# Load the model\n",
        "mobile_sam = sam_model_registry[model_type](args , checkpoint=sam_checkpoint)\n",
        "# Move the model to the device\n",
        "mobile_sam = mobile_sam.to(device=device)\n",
        "# Set the model to evaluation mode\n",
        "mobile_sam.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0879, -0.1321,  0.0836,  ...,  0.0004, -0.0185,  0.0011],\n",
              "        [-0.0375, -0.1283,  0.0036,  ...,  0.0273, -0.0380,  0.0483],\n",
              "        [-0.0320,  0.0465,  0.0535,  ..., -0.0090, -0.0755,  0.1173],\n",
              "        ...,\n",
              "        [ 0.0465,  0.0303, -0.0648,  ...,  0.0008,  0.0163, -0.0158],\n",
              "        [ 0.0622,  0.0186, -0.0068,  ...,  0.0297,  0.0375, -0.0383],\n",
              "        [ 0.0084, -0.0070,  0.0405,  ..., -0.0055,  0.0015,  0.0230]],\n",
              "       device='cuda:0', requires_grad=True)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next( mobile_sam.mask_decoder.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from segment_anything import SamPredictor, sam_model_registry\n",
        "from models.sam import SamPredictor, sam_model_registry\n",
        "from models.sam.utils.transforms import ResizeLongestSide\n",
        "from skimage.measure import label\n",
        "from models.sam_LoRa import LoRA_Sam\n",
        "#Scientific computing \n",
        "import numpy as np\n",
        "import os\n",
        "#Pytorch packages\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "#from tensorboardX import SummaryWriter\n",
        "#Visulization\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "#Others\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from utils.dataset import Public_dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from utils.losses import DiceLoss\n",
        "from utils.dsc import dice_coeff_multi_class\n",
        "import cv2\n",
        "import monai\n",
        "#from utils.utils import vis_image\n",
        "import cfg\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\build_sam.py:157: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sam(\n",
              "  (image_encoder): TinyViT(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (seq): Sequential(\n",
              "        (0): Conv2d_BN(\n",
              "          (c): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Conv2d_BN(\n",
              "          (c): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): ConvLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x MBConv(\n",
              "            (conv1): Conv2d_BN(\n",
              "              (c): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d_BN(\n",
              "              (c): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act2): GELU(approximate='none')\n",
              "            (conv3): Conv2d_BN(\n",
              "              (c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "            (act3): GELU(approximate='none')\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (downsample): PatchMerging(\n",
              "          (act): GELU(approximate='none')\n",
              "          (conv1): Conv2d_BN(\n",
              "            (c): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv2): Conv2d_BN(\n",
              "            (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv3): Conv2d_BN(\n",
              "            (c): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): BasicLayer(\n",
              "        dim=128, input_resolution=(128, 128), depth=2\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x TinyViTBlock(\n",
              "            dim=128, input_resolution=(128, 128), num_heads=4, window_size=7, mlp_ratio=4.0\n",
              "            (drop_path): Identity()\n",
              "            (attn): Attention(\n",
              "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "            )\n",
              "            (mlp): Mlp(\n",
              "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
              "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (local_conv): Conv2d_BN(\n",
              "              (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (downsample): PatchMerging(\n",
              "          (act): GELU(approximate='none')\n",
              "          (conv1): Conv2d_BN(\n",
              "            (c): Conv2d(128, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv2): Conv2d_BN(\n",
              "            (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)\n",
              "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv3): Conv2d_BN(\n",
              "            (c): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): BasicLayer(\n",
              "        dim=160, input_resolution=(64, 64), depth=6\n",
              "        (blocks): ModuleList(\n",
              "          (0-5): 6 x TinyViTBlock(\n",
              "            dim=160, input_resolution=(64, 64), num_heads=5, window_size=14, mlp_ratio=4.0\n",
              "            (drop_path): Identity()\n",
              "            (attn): Attention(\n",
              "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
              "              (proj): Linear(in_features=160, out_features=160, bias=True)\n",
              "            )\n",
              "            (mlp): Mlp(\n",
              "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
              "              (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (local_conv): Conv2d_BN(\n",
              "              (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
              "              (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (downsample): PatchMerging(\n",
              "          (act): GELU(approximate='none')\n",
              "          (conv1): Conv2d_BN(\n",
              "            (c): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv2): Conv2d_BN(\n",
              "            (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
              "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (conv3): Conv2d_BN(\n",
              "            (c): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): BasicLayer(\n",
              "        dim=320, input_resolution=(64, 64), depth=2\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x TinyViTBlock(\n",
              "            dim=320, input_resolution=(64, 64), num_heads=10, window_size=7, mlp_ratio=4.0\n",
              "            (drop_path): Identity()\n",
              "            (attn): Attention(\n",
              "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
              "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
              "            )\n",
              "            (mlp): Mlp(\n",
              "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
              "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (local_conv): Conv2d_BN(\n",
              "              (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
              "              (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm_head): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "    (head): Linear(in_features=320, out_features=1000, bias=True)\n",
              "    (neck): Sequential(\n",
              "      (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): LayerNorm2d()\n",
              "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (3): LayerNorm2d()\n",
              "    )\n",
              "  )\n",
              "  (prompt_encoder): PromptEncoder(\n",
              "    (pe_layer): PositionEmbeddingRandom()\n",
              "    (point_embeddings): ModuleList(\n",
              "      (0-3): 4 x Embedding(1, 256)\n",
              "    )\n",
              "    (not_a_point_embed): Embedding(1, 256)\n",
              "    (mask_downscaling): Sequential(\n",
              "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): LayerNorm2d()\n",
              "      (5): GELU(approximate='none')\n",
              "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (no_mask_embed): Embedding(1, 256)\n",
              "  )\n",
              "  (mask_decoder): MaskDecoder(\n",
              "    (transformer): TwoWayTransformer(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x TwoWayAttentionBlock(\n",
              "          (self_attn): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_token_to_image): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (act): ReLU()\n",
              "          )\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_image_to_token): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_attn_token_to_image): Attention(\n",
              "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (iou_token): Embedding(1, 256)\n",
              "    (mask_tokens): Embedding(3, 256)\n",
              "    (output_upscaling): Sequential(\n",
              "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): GELU(approximate='none')\n",
              "    )\n",
              "    (output_hypernetworks_mlps): ModuleList(\n",
              "      (0-2): 3 x MLP(\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (iou_prediction_head): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=3, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args.arch = \"vit_t\"\n",
        "args.sam_ckpt = \"mobile_sam_weights.pt\"\n",
        "args.finetune_type = \"vanilla\"\n",
        "sam = sam_model_registry[args.arch](args,checkpoint=sam_checkpoint,num_classes=args.num_cls)\n",
        "\n",
        "\n",
        "# args.arch = \"vit_b\"\n",
        "# args.sam_ckpt = \"sam_vit_b_01ec64.pth\" \n",
        "# sam = sam_model_registry[args.arch](args,checkpoint=os.path.join(args.sam_ckpt),num_classes=args.num_cls)\n",
        "\n",
        "sam.eval()\n",
        "sam.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "sam = mobile_sam\n",
        "from models.sam import predictor\n",
        "predictor = predictor.SamPredictor(sam)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def put_mark(img, center, size):\n",
        "    img = img.copy()\n",
        "    x, y = center\n",
        "    x = int(x)\n",
        "    y = int(y)\n",
        "    size = int(size)\n",
        "    img[y-size:y+size, x-size:x+size] = [0, 255, 0]\n",
        "    \n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1024, 1024, 3)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "image = Image.open('picture4.jpg')\n",
        "image = np.asarray(image)\n",
        "image = image[0:1024,0:1024]\n",
        "image = image.copy()\n",
        "image = put_mark(image, (940,730), 5)\n",
        "print(image.shape)\n",
        "Image.fromarray(image).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_results(masks, scores, logits, filter=0):\n",
        "    annotations = []\n",
        "    n = len(scores)\n",
        "    for i in range(n):\n",
        "        annotation = {}\n",
        "\n",
        "        mask = masks[i]\n",
        "        tmp = np.where(mask != 0)\n",
        "        if np.sum(mask) < filter:\n",
        "            continue\n",
        "        annotation[\"id\"] = i\n",
        "        annotation[\"segmentation\"] = mask\n",
        "        annotation[\"bbox\"] = [\n",
        "            np.min(tmp[0]),\n",
        "            np.min(tmp[1]),\n",
        "            np.max(tmp[1]),\n",
        "            np.max(tmp[0]),\n",
        "        ]\n",
        "        annotation[\"score\"] = scores[i]\n",
        "        annotation[\"area\"] = annotation[\"segmentation\"].sum()\n",
        "        annotations.append(annotation)\n",
        "    return annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def put_massk(image,mask):\n",
        "    img = image.copy()\n",
        "    for i in range(mask.shape[0]):\n",
        "        for j in range(mask.shape[1]):\n",
        "            if mask[i,j] == 1:\n",
        "                img[i,j] = [255,0,0]\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\predictor.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_image = torch.tensor(transformed_image*1.0/255)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "p = np.array([940,730]).reshape(1,2)\n",
        "p = None\n",
        "\n",
        "predictor.set_image(image)\n",
        "masks, scores, logits = predictor.predict(p,point_labels= np.array([1]))\n",
        "results = format_results(masks, scores, logits, 0)\n",
        "\n",
        "fst = results[0][\"segmentation\"]\n",
        "#fst,snd= results[0][\"segmentation\"],results[1][\"segmentation\"]#,results[2][\"segmentation\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.02837351, -0.09287645], dtype=float32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'snd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(put_massk(image,fst))\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m----> 2\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(put_massk(image,\u001b[43msnd\u001b[49m))\u001b[38;5;241m.\u001b[39mshow()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'snd' is not defined"
          ]
        }
      ],
      "source": [
        "Image.fromarray(put_massk(image,fst)).show()\n",
        "Image.fromarray(put_massk(image,snd)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'optim' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[36], line 29\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#def train_model(trainloader,valloader,dir_checkpoint,epochs):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# if args.if_warmup:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     b_lr = args.lr / args.warmup_period\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#     sam = LoRA_Sam(args,sam,r=4).sam\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# sam.to('cuda')\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241m.\u001b[39mAdamW(sam\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mb_lr, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-08\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, amsgrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;66;03m#learning rate decay\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'optim' is not defined"
          ]
        }
      ],
      "source": [
        "#def train_model(trainloader,valloader,dir_checkpoint,epochs):\n",
        "# if args.if_warmup:\n",
        "#     b_lr = args.lr / args.warmup_period\n",
        "# else:\n",
        "#     b_lr = args.lr\n",
        "\n",
        "#sam = sam_model_registry[args.arch](args,checkpoint=os.path.join(args.sam_ckpt),num_classes=args.num_cls)\n",
        "# if args.finetune_type == 'adapter':\n",
        "#     for n, value in sam.named_parameters():\n",
        "#         if \"Adapter\" not in n: # only update parameters in adapter\n",
        "#             value.requires_grad = False\n",
        "#     print('if update encoder:',args.if_update_encoder)\n",
        "#     print('if image encoder adapter:',args.if_encoder_adapter)\n",
        "#     print('if mask decoder adapter:',args.if_mask_decoder_adapter)\n",
        "#     if args.if_encoder_adapter:\n",
        "#         print('added adapter layers:',args.encoder_adapter_depths)\n",
        "    \n",
        "# elif args.finetune_type == 'vanilla' and args.if_update_encoder==False:   \n",
        "#     print('if update encoder:',args.if_update_encoder)\n",
        "#     for n, value in sam.image_encoder.named_parameters():\n",
        "#         value.requires_grad = False\n",
        "# elif args.finetune_type == 'lora':\n",
        "#     print('if update encoder:',args.if_update_encoder)\n",
        "#     print('if image encoder lora:',args.if_encoder_lora_layer)\n",
        "#     print('if mask decoder lora:',args.if_decoder_lora_layer)\n",
        "#     sam = LoRA_Sam(args,sam,r=4).sam\n",
        "# sam.to('cuda')\n",
        "    \n",
        "optimizer = optim.AdamW(sam.parameters(), lr=b_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1, amsgrad=False)\n",
        "optimizer.zero_grad()\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5) #learning rate decay\n",
        "criterion1 = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, to_onehot_y=True,reduction='mean')\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "iter_num = 0\n",
        "max_iterations = epochs * len(trainloader) \n",
        "writer = SummaryWriter(dir_checkpoint + '/log')\n",
        "\n",
        "pbar = tqdm(range(epochs))\n",
        "val_largest_dsc = 0\n",
        "last_update_epoch = 0\n",
        "for epoch in pbar:\n",
        "    sam.train()\n",
        "    train_loss = 0\n",
        "    for i,data in enumerate(tqdm(trainloader)):\n",
        "        imgs = data['image'].cuda()\n",
        "        msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n",
        "        msks = msks.cuda()\n",
        "\n",
        "        if args.if_update_encoder:\n",
        "            img_emb = sam.image_encoder(imgs)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                img_emb = sam.image_encoder(imgs)\n",
        "        \n",
        "        # get default embeddings\n",
        "        sparse_emb, dense_emb = sam.prompt_encoder(\n",
        "            points=None,\n",
        "            boxes=None,\n",
        "            masks=None,\n",
        "        )\n",
        "        pred, _ = sam.mask_decoder(\n",
        "                        image_embeddings=img_emb,\n",
        "                        image_pe=sam.prompt_encoder.get_dense_pe(), \n",
        "                        sparse_prompt_embeddings=sparse_emb,\n",
        "                        dense_prompt_embeddings=dense_emb, \n",
        "                        multimask_output=True,\n",
        "                        )\n",
        "        loss_dice = criterion1(pred,msks.float()) \n",
        "        loss_ce = criterion2(pred,torch.squeeze(msks.long(),1))\n",
        "        loss =  loss_dice + loss_ce\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        if args.if_warmup and iter_num < args.warmup_period:\n",
        "            lr_ = args.lr * ((iter_num + 1) / args.warmup_period)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_\n",
        "\n",
        "        else:\n",
        "            if args.if_warmup:\n",
        "                shift_iter = iter_num - args.warmup_period\n",
        "                assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
        "                lr_ = args.lr * (1.0 - shift_iter / max_iterations) ** 0.9  # learning rate adjustment depends on the max iterations\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr_\n",
        "            else:\n",
        "                lr_ = args.lr\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        iter_num+=1\n",
        "        writer.add_scalar('info/lr', lr_, iter_num)\n",
        "        writer.add_scalar('info/total_loss', loss, iter_num)\n",
        "        writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n",
        "        writer.add_scalar('info/loss_dice', loss_dice, iter_num)\n",
        "\n",
        "    train_loss /= (i+1)\n",
        "    pbar.set_description('Epoch num {}| train loss {} \\n'.format(epoch,train_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 1024, 1024])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "expected 4D input (got 3D input)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmobile_sam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreproc_img\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\KU\\ATIA\\MobileSAM\\mobile_sam\\modeling\\tiny_vit_sam.py:617\u001b[0m, in \u001b[0;36mTinyViT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 617\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;66;03m#x = self.norm_head(x)\u001b[39;00m\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;66;03m#x = self.head(x)\u001b[39;00m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\KU\\ATIA\\MobileSAM\\mobile_sam\\modeling\\tiny_vit_sam.py:602\u001b[0m, in \u001b[0;36mTinyViT.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;66;03m# x: (N, C, H, W)\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](x)\n\u001b[0;32m    605\u001b[0m     start_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\KU\\ATIA\\MobileSAM\\mobile_sam\\modeling\\tiny_vit_sam.py:74\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:160\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:452\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_input_dim\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 452\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 4D input (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD input)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: expected 4D input (got 3D input)"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a3663b3d63a4c4cac421d8fd4746051": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ffd23cd526d49bbb04bb3fb3713d38c",
            "placeholder": "​",
            "style": "IPY_MODEL_e3bd26913d7d48ba8622d17d290ed9b9",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "4988b977dc674fde92fe781df557bd9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "762364a1ca11410894f2ba1104b3b81f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0caf5e9e72d4b029a7bb4d10baed9e8",
            "placeholder": "​",
            "style": "IPY_MODEL_937b51bfed124a47a46c4773998531f5",
            "value": " 466/466 [00:00&lt;00:00, 33.8kB/s]"
          }
        },
        "7ab0cc445a37426c89c5710c15db6227": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a3663b3d63a4c4cac421d8fd4746051",
              "IPY_MODEL_ab8c2f638a7e49bc95f5f3675cdfb737",
              "IPY_MODEL_762364a1ca11410894f2ba1104b3b81f"
            ],
            "layout": "IPY_MODEL_8aa7ec419c4a4c7db62e6fcc1df27498"
          }
        },
        "7ffd23cd526d49bbb04bb3fb3713d38c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa7ec419c4a4c7db62e6fcc1df27498": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "937b51bfed124a47a46c4773998531f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0caf5e9e72d4b029a7bb4d10baed9e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab8c2f638a7e49bc95f5f3675cdfb737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9e5b3a11c642c5b32b84a693751ac3",
            "max": 466,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4988b977dc674fde92fe781df557bd9a",
            "value": 466
          }
        },
        "df9e5b3a11c642c5b32b84a693751ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3bd26913d7d48ba8622d17d290ed9b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
