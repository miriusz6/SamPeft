{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HAWkO7kwZUSQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from skimage.measure import label\n",
        "\n",
        "#Scientific computing \n",
        "import numpy as np\n",
        "import os\n",
        "#Pytorch packages\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "#from torchvision import datasets\n",
        "from tensorboardX import SummaryWriter\n",
        "#Visulization\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "#Others\n",
        "#from torch.utils.data import DataLoader, Subset\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "#from finetuneSAM.utils.dataset import Public_dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "#from finetuneSAM.utils.losses import DiceLoss\n",
        "#from finetuneSAM.utils.dsc import dice_coeff_multi_class\n",
        "import cv2\n",
        "import monai\n",
        "#from finetuneSAM.utils.utils import vis_image\n",
        "#import cfg\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "# General\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "from skimage.measure import label\n",
        "\n",
        "#Scientific computing \n",
        "import numpy as np\n",
        "import os\n",
        "#Pytorch packages\n",
        "import torch\n",
        "\n",
        "import torchvision\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "c:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_5m_224 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_11m_224 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_224 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_384 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_512 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\build_sam.py:157: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        }
      ],
      "source": [
        "#Fine-Tune Sam\n",
        "from finetuneSAM.models.sam import SamAutomaticMaskGenerator, SamPredictor, sam_model_registry\n",
        "from finetuneSAM.cfg import parse_args\n",
        "args =  parse_args()\n",
        "\n",
        "\n",
        "# setting if_mask_decoder_adapter = True puts adapters inside 2-way transformer blocks\n",
        "# this does not change the number of decoder 2-way transformer blocks (def = 2)\n",
        "# decoder_adapt_depth denotes how many of the two 2-way transformer blocks are adapted\n",
        "\n",
        "\n",
        "# setting if_encoder_adapter = True puts adapters inside TinyViTBlocks in the encoder\n",
        "# this does not change the number of encoder TinyViTBlocks (def = 4)\n",
        "# encoder_adapt_depth (e.g. [1,2]) denotes how deep blocks will be adapted\n",
        "\n",
        "args.finetune_type = \"vanilla\"\n",
        "#args.finetune_type = \"adapter\"# \"vanilla\"\n",
        "#args.if_mask_decoder_adapter = True\n",
        "#args.image_size = 512\n",
        "#args.decoder_adapt_depth = 1\n",
        "args.num_cls = 3\n",
        "\n",
        "\n",
        "# Load the pre-trained model\n",
        "sam_checkpoint = \"mobile_sam.pt\"\n",
        "# Define the model type: Tiny Vit\n",
        "model_type = \"vit_t\"\n",
        "\n",
        "# Load the model\n",
        "mobile_sam_f = sam_model_registry[model_type](args , checkpoint=sam_checkpoint)\n",
        "# Move the model to the device\n",
        "mobile_sam_f = mobile_sam_f.to(device=device)\n",
        "# Set the model to evaluation mode\n",
        "#mobile_sam_f.eval()\n",
        "\n",
        "predictor_f = SamPredictor(mobile_sam_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from finetuneSAM.models.sam.utils.transforms import ResizeLongestSide\n",
        "\n",
        "def preprocess(model, input_img_size, image: np.ndarray, point = None):\n",
        "    torch.no_grad()\n",
        "    org_shape = image.shape\n",
        "\n",
        "    transform = ResizeLongestSide(input_img_size) # can be changed?\n",
        "\n",
        "    #set_image\n",
        "    input_image = transform.apply_image(image)\n",
        "    input_image_torch = torch.as_tensor(input_image, device=device)\n",
        "    input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
        "\n",
        "    #set_torch_image\n",
        "    transformed_image = input_image_torch\n",
        "    original_image_size = input_image.shape[:2]\n",
        "    input_size = tuple(transformed_image.shape[-2:])\n",
        "    transformed_image = model.preprocess(transformed_image)\n",
        "    #input_image = torch.tensor(transformed_image*1.0/255) \n",
        "    features = model.image_encoder(transformed_image)\n",
        "\n",
        "\n",
        "    coords_torch, labels_torch, box_torch, mask_input_torch = None, None, None, None\n",
        "    # masks, iou_predictions, low_res_masks = self.predict_torch(\n",
        "    #     coords_torch,\n",
        "    #     labels_torch,\n",
        "    #     box_torch,\n",
        "    #     mask_input_torch,\n",
        "    #     multimask_output,\n",
        "    #     return_logits=return_logits,\n",
        "    # )\n",
        "\n",
        "    point_coords = transform.apply_coords(np.array([[940, 730]]).reshape(1,2), org_shape[:2])\n",
        "    coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=device)\n",
        "    labels_torch = torch.as_tensor(np.array([1]), dtype=torch.int, device=device)\n",
        "    coords_torch, labels_torch = coords_torch[None, :, :], labels_torch[None, :]\n",
        "\n",
        "\n",
        "    points=(coords_torch, labels_torch)\n",
        "    sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
        "            points=points,\n",
        "            boxes=box_torch,\n",
        "            masks=mask_input_torch,\n",
        "        )\n",
        "\n",
        "    # Predict masks\n",
        "    low_res_masks, iou_predictions = model.mask_decoder(\n",
        "        image_embeddings = features,\n",
        "        image_pe= model.prompt_encoder.get_dense_pe(),\n",
        "        sparse_prompt_embeddings=sparse_embeddings,\n",
        "        dense_prompt_embeddings=dense_embeddings,\n",
        "        multimask_output= True,\n",
        "    )\n",
        "\n",
        "    # Upscale the masks to the original image resolution\n",
        "    masks = model.postprocess_masks(low_res_masks, input_size, org_shape[:2])# original_image_size)\n",
        "    \n",
        "    #masks = masks > 0.0\n",
        "\n",
        "    # if not [0] then batched ?\n",
        "    return masks[0].detach().cpu().numpy(), iou_predictions[0].detach().cpu().numpy(), low_res_masks[0].detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2048, 1365)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1365, 2048, 3)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image = Image.open('picture4.jpg')\n",
        "print(image.size)\n",
        "image = np.asarray(image)\n",
        "#image = image[0:254,0:254]\n",
        "#image = image[0:1024,0:1024]\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input identical True\n",
            "input_image_torch(after cast):  True\n",
            "input_image_torch (after permute)  True\n",
            "input_image after preproc:  True\n",
            "features identical True\n"
          ]
        }
      ],
      "source": [
        "predictor_f.set_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_results(masks, scores, logits, filter=0):\n",
        "    annotations = []\n",
        "    n = len(scores)\n",
        "    for i in range(n):\n",
        "        annotation = {}\n",
        "\n",
        "        mask = masks[i]\n",
        "        tmp = np.where(mask != 0)\n",
        "        if np.sum(mask) < filter:\n",
        "            continue\n",
        "        annotation[\"id\"] = i\n",
        "        annotation[\"segmentation\"] = mask\n",
        "        annotation[\"bbox\"] = [\n",
        "            np.min(tmp[0]),\n",
        "            np.min(tmp[1]),\n",
        "            np.max(tmp[1]),\n",
        "            np.max(tmp[0]),\n",
        "        ]\n",
        "        annotation[\"score\"] = scores[i]\n",
        "        annotation[\"area\"] = annotation[\"segmentation\"].sum()\n",
        "        annotations.append(annotation)\n",
        "    return annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 1365, 2048)\n",
            "[0.85441417 1.0176673  1.0066909 ]\n"
          ]
        }
      ],
      "source": [
        "masks, scores, logits = preprocess(mobile_sam_f, 1024, image)\n",
        "print(masks.shape)\n",
        "print(scores)\n",
        "#results = format_results(masks, scores, logits, filter=0)\n",
        "#results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def put_massk(image,mask):\n",
        "    img = image.copy()\n",
        "    for i in range(mask.shape[0]):\n",
        "        for j in range(mask.shape[1]):\n",
        "            if mask[i,j] == 1:\n",
        "                img[i,j] = [255,0,0]\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "r = put_massk(image,masks[1])\n",
        "Image.fromarray(r).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "m1 = logits[2]\n",
        "m = 1* m1\n",
        "m = m*255\n",
        "\n",
        "#set rgb to 255 for all 1s in the mask\n",
        "\n",
        "Image.fromarray(m).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Image.fromarray(results[0][\"segmentation\"]).show()\n",
        "#Image.fromarray(results[0][\"segmentation\"]).show()\n",
        "\n",
        "m1 = masks[1]\n",
        "m = 1* m1\n",
        "m = m*255\n",
        "\n",
        "#set rgb to 255 for all 1s in the mask\n",
        "\n",
        "Image.fromarray(m).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input identical True\n",
            "input_image_torch(after cast):  True\n",
            "input_image_torch (after permute)  True\n",
            "input_image after preproc:  True\n",
            "features identical True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.84140253, 0.9449352 , 0.8273449 ], dtype=float32)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictor_f.set_image(image)\n",
        "FEAT_AUTO = predictor_f.features\n",
        "#masks, scores, logits = predictor_f.predict(None,None)#, multimask_output=True)\n",
        "\n",
        "p = np.array([940,730]).reshape(1,2)\n",
        "masks, scores, logits = predictor_f.predict(p,point_labels= np.array([1]))\n",
        "# p,point_labels= np.array([1])\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = put_massk(image,masks[2])\n",
        "Image.fromarray(q).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dice_coeff_multi_class(pred, target, n_classes):\n",
        "    \"\"\"Calculate the mean Dice Coefficient for multi-class data.\"\"\"\n",
        "    dice_scores = []\n",
        "    for cls in range(n_classes):  # Iterate over each class\n",
        "        pred_cls = (pred == cls).long()  # Binary mask for current predicted class\n",
        "        target_cls = (target == cls).long()  # Binary mask for current actual class\n",
        "\n",
        "        smooth = 1.0\n",
        "        intersection = (pred_cls & target_cls).float().sum((1, 2))  # Sum over height and width dimensions\n",
        "        union = pred_cls.float().sum((1, 2)) + target_cls.float().sum((1, 2))\n",
        "        \n",
        "        dice = (2. * intersection + smooth) / (union + smooth)\n",
        "        dice_scores.append(dice)\n",
        "    \n",
        "    return torch.stack(dice_scores).mean() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "if update encoder: False\n",
            "if image encoder adapter: False\n",
            "if mask decoder adapter: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/80 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 1024, 1024, 3] to have 3 channels, but got 1024 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 59\u001b[0m         img_emb \u001b[38;5;241m=\u001b[39m \u001b[43msam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# get default embeddings\u001b[39;00m\n\u001b[0;32m     62\u001b[0m sparse_emb, dense_emb \u001b[38;5;241m=\u001b[39m sam\u001b[38;5;241m.\u001b[39mprompt_encoder(\n\u001b[0;32m     63\u001b[0m     points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     64\u001b[0m     boxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     65\u001b[0m     masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     66\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:721\u001b[0m, in \u001b[0;36mTinyViT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 721\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m#x = self.norm_head(x)\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;66;03m#x = self.head(x)\u001b[39;00m\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:704\u001b[0m, in \u001b[0;36mTinyViT.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;66;03m# x: (N, C, H, W)\u001b[39;00m\n\u001b[1;32m--> 704\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](x)\n\u001b[0;32m    707\u001b[0m     start_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:91\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 1024, 1024, 3] to have 3 channels, but got 1024 channels instead"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "sam = mobile_sam_f\n",
        "\n",
        "data = {'image':train_imgs, 'mask':train_masks}\n",
        "\n",
        "\n",
        "if args.finetune_type == 'adapter':\n",
        "    for n, value in sam.named_parameters():\n",
        "        if \"Adapter\" not in n: # only update parameters in adapter\n",
        "            value.requires_grad = False\n",
        "    print('if update encoder:',args.if_update_encoder)\n",
        "    print('if image encoder adapter:',args.if_encoder_adapter)\n",
        "    print('if mask decoder adapter:',args.if_mask_decoder_adapter)\n",
        "    if args.if_encoder_adapter:\n",
        "        print('added adapter layers:',args.encoder_adapter_depths)\n",
        "    \n",
        "# elif args.finetune_type == 'vanilla' and args.if_update_encoder==False:   \n",
        "#     print('if update encoder:',args.if_update_encoder)\n",
        "#     for n, value in sam.image_encoder.named_parameters():\n",
        "#         value.requires_grad = False\n",
        "# elif args.finetune_type == 'lora':\n",
        "#     print('if update encoder:',args.if_update_encoder)\n",
        "#     print('if image encoder lora:',args.if_encoder_lora_layer)\n",
        "#     print('if mask decoder lora:',args.if_decoder_lora_layer)\n",
        "#     sam = LoRA_Sam(args,sam,r=4).sam\n",
        "# sam.to('cuda')\n",
        "b_lr = args.lr\n",
        "epochs = 80\n",
        "    \n",
        "optimizer = optim.AdamW(sam.parameters(), lr=b_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1, amsgrad=False)\n",
        "optimizer.zero_grad()\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5) #learning rate decay\n",
        "criterion1 = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, to_onehot_y=True,reduction='mean')\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "iter_num = 0\n",
        "max_iterations = epochs * train_points\n",
        "writer = SummaryWriter(checkpoints_path + '/log')\n",
        "\n",
        "pbar = tqdm(range(epochs))\n",
        "val_largest_dsc = 0\n",
        "last_update_epoch = 0\n",
        "for epoch in pbar:\n",
        "\n",
        "\n",
        "    sam.train()\n",
        "    train_loss = 0\n",
        "    # batches here?\n",
        "    #for i,_ in enumerate(tqdm(data['image'])):\n",
        "    for i in range(1):\n",
        "        imgs = data['image'][0:1].cuda()\n",
        "        msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'][0:1])\n",
        "        msks = msks.cuda()\n",
        "\n",
        "\n",
        "\n",
        "        if args.if_update_encoder:\n",
        "            img_emb = sam.image_encoder(imgs)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                img_emb = sam.image_encoder(imgs)\n",
        "        \n",
        "        # get default embeddings\n",
        "        sparse_emb, dense_emb = sam.prompt_encoder(\n",
        "            points=None,\n",
        "            boxes=None,\n",
        "            masks=None,\n",
        "        )\n",
        "        pred, _ = sam.mask_decoder(\n",
        "                        image_embeddings=img_emb,\n",
        "                        image_pe=sam.prompt_encoder.get_dense_pe(), \n",
        "                        sparse_prompt_embeddings=sparse_emb,\n",
        "                        dense_prompt_embeddings=dense_emb, \n",
        "                        #multimask_output=True,\n",
        "                        multimask_output=False,\n",
        "                        )\n",
        "        \n",
        "        print(\"THIS WORKS\")\n",
        "        print(msks.shape)\n",
        "        print(msks.float().shape)\n",
        "        print(pred.shape)\n",
        "        \n",
        "\n",
        "\n",
        "        #loss_dice = criterion1(pred,msks.float()) \n",
        "        loss_ce = criterion2(pred,torch.squeeze(msks.long(),1))\n",
        "        loss =  loss_ce\n",
        "        #loss =  loss_dice + loss_ce\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        if args.if_warmup and iter_num < args.warmup_period:\n",
        "            lr_ = args.lr * ((iter_num + 1) / args.warmup_period)\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_\n",
        "\n",
        "        else:\n",
        "            if args.if_warmup:\n",
        "                shift_iter = iter_num - args.warmup_period\n",
        "                assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
        "                lr_ = args.lr * (1.0 - shift_iter / max_iterations) ** 0.9  # learning rate adjustment depends on the max iterations\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr_\n",
        "            else:\n",
        "                lr_ = args.lr\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        iter_num+=1\n",
        "        writer.add_scalar('info/lr', lr_, iter_num)\n",
        "        writer.add_scalar('info/total_loss', loss, iter_num)\n",
        "        writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n",
        "        writer.add_scalar('info/loss_dice', loss_dice, iter_num)\n",
        "\n",
        "    train_loss /= (i+1)\n",
        "    pbar.set_description('Epoch num {}| train loss {} \\n'.format(epoch,train_loss))\n",
        "\n",
        "    # if epoch%2==0:\n",
        "    #     eval_loss=0\n",
        "    #     dsc = 0\n",
        "    #     sam.eval()\n",
        "    #     with torch.no_grad():\n",
        "    #         for i in tqdm(data['image']):\n",
        "    #             imgs = data['image'][i].cuda()\n",
        "    #             msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'][i])\n",
        "    #             msks = msks.cuda()\n",
        "\n",
        "    #             img_emb= sam.image_encoder(imgs)\n",
        "    #             sparse_emb, dense_emb = sam.prompt_encoder(\n",
        "    #                 points=None,\n",
        "    #                 boxes=None,\n",
        "    #                 masks=None,\n",
        "    #             )\n",
        "    #             pred, _ = sam.mask_decoder(\n",
        "    #                             image_embeddings=img_emb,\n",
        "    #                             image_pe=sam.prompt_encoder.get_dense_pe(), \n",
        "    #                             sparse_prompt_embeddings=sparse_emb,\n",
        "    #                             dense_prompt_embeddings=dense_emb, \n",
        "    #                             multimask_output=True,\n",
        "    #                             )\n",
        "    #             loss = criterion1(pred,msks.float()) + criterion2(pred,torch.squeeze(msks.long(),1))\n",
        "    #             eval_loss +=loss.item()\n",
        "    #             dsc_batch = dice_coeff_multi_class(pred.argmax(dim=1).cpu(), torch.squeeze(msks.long(),1).cpu().long(),args.num_cls)\n",
        "    #             dsc+=dsc_batch\n",
        "\n",
        "    #         eval_loss /= (i+1)\n",
        "    #         dsc /= (i+1)\n",
        "            \n",
        "    #         writer.add_scalar('eval/loss', eval_loss, epoch)\n",
        "    #         writer.add_scalar('eval/dice', dsc, epoch)\n",
        "            \n",
        "    #         print('Eval Epoch num {} | val loss {} | dsc {} \\n'.format(epoch,eval_loss,dsc))\n",
        "    #         if dsc>val_largest_dsc:\n",
        "    #             val_largest_dsc = dsc\n",
        "    #             last_update_epoch = epoch\n",
        "    #             print('largest DSC now: {}'.format(dsc))\n",
        "    #             torch.save(sam.state_dict(),checkpoints_path + '/checkpoint_best.pth')\n",
        "    #         elif (epoch-last_update_epoch)>20:\n",
        "    #             # the network haven't been updated for 20 epochs\n",
        "    #             print('Training finished###########')\n",
        "    #             break\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a3663b3d63a4c4cac421d8fd4746051": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ffd23cd526d49bbb04bb3fb3713d38c",
            "placeholder": "​",
            "style": "IPY_MODEL_e3bd26913d7d48ba8622d17d290ed9b9",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "4988b977dc674fde92fe781df557bd9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "762364a1ca11410894f2ba1104b3b81f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0caf5e9e72d4b029a7bb4d10baed9e8",
            "placeholder": "​",
            "style": "IPY_MODEL_937b51bfed124a47a46c4773998531f5",
            "value": " 466/466 [00:00&lt;00:00, 33.8kB/s]"
          }
        },
        "7ab0cc445a37426c89c5710c15db6227": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a3663b3d63a4c4cac421d8fd4746051",
              "IPY_MODEL_ab8c2f638a7e49bc95f5f3675cdfb737",
              "IPY_MODEL_762364a1ca11410894f2ba1104b3b81f"
            ],
            "layout": "IPY_MODEL_8aa7ec419c4a4c7db62e6fcc1df27498"
          }
        },
        "7ffd23cd526d49bbb04bb3fb3713d38c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa7ec419c4a4c7db62e6fcc1df27498": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "937b51bfed124a47a46c4773998531f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0caf5e9e72d4b029a7bb4d10baed9e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab8c2f638a7e49bc95f5f3675cdfb737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9e5b3a11c642c5b32b84a693751ac3",
            "max": 466,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4988b977dc674fde92fe781df557bd9a",
            "value": 466
          }
        },
        "df9e5b3a11c642c5b32b84a693751ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3bd26913d7d48ba8622d17d290ed9b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
