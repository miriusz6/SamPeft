{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HAWkO7kwZUSQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "c:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_5m_224 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_11m_224 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_224 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_384 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_512 in registry with finetuneSAM.models.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from skimage.measure import label\n",
    "\n",
    "#Scientific computing \n",
    "import numpy as np\n",
    "import os\n",
    "#Pytorch packages\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "#from torchvision import datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "#Visulization\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "#Others\n",
    "#from torch.utils.data import DataLoader, Subset\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "#from finetuneSAM.utils.dataset import Public_dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import one_hot\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "#from finetuneSAM.utils.losses import DiceLoss\n",
    "#from finetuneSAM.utils.dsc import dice_coeff_multi_class\n",
    "import cv2\n",
    "import monai\n",
    "#from finetuneSAM.utils.utils import vis_image\n",
    "\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from skimage.measure import label\n",
    "\n",
    "#Scientific computing \n",
    "import numpy as np\n",
    "import os\n",
    "#Pytorch packages\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from finetuneSAM.models.sam.utils.transforms import ResizeLongestSide\n",
    "from finetuneSAM.models.sam import SamPredictor, sam_model_registry\n",
    "from finetuneSAM.cfg import parse_args\n",
    "\n",
    "from mmutils import put_marks, put_mask\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KU\\ATIA\\finetuneSAM\\models\\sam\\build_sam.py:158: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "#Fine-Tune Sam\n",
    "args =  parse_args()\n",
    "\n",
    "\n",
    "# setting if_mask_decoder_adapter = True puts adapters inside 2-way transformer blocks\n",
    "# this does not change the number of decoder 2-way transformer blocks (def = 2)\n",
    "# decoder_adapt_depth denotes how many of the two 2-way transformer blocks are adapted\n",
    "\n",
    "\n",
    "# setting if_encoder_adapter = True puts adapters inside TinyViTBlocks in the encoder\n",
    "# this does not change the number of encoder TinyViTBlocks (def = 4)\n",
    "# encoder_adapt_depth (e.g. [1,2]) denotes how deep blocks will be adapted\n",
    "\n",
    "args.finetune_type = \"vanilla\"\n",
    "#args.finetune_type = \"adapter\"# \"vanilla\"\n",
    "#args.if_mask_decoder_adapter = True\n",
    "#args.image_size = 512\n",
    "#args.decoder_adapt_depth = 1\n",
    "args.num_cls = 2\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "sam_checkpoint = \"mobile_sam.pt\"\n",
    "# Define the model type: Tiny Vit\n",
    "model_type = \"vit_t\"\n",
    "\n",
    "# Load the model\n",
    "mobile_sam_f = sam_model_registry[model_type](args , checkpoint=sam_checkpoint)\n",
    "# Move the model to the device\n",
    "mobile_sam_f = mobile_sam_f.to(device=device)\n",
    "# Set the model to evaluation mode\n",
    "#mobile_sam_f.eval()\n",
    "\n",
    "predictor_f = SamPredictor(mobile_sam_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predcit def:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_img_size, image: np.ndarray, point = None):\n",
    "    torch.no_grad()\n",
    "    org_shape = image.shape\n",
    "\n",
    "    transform = ResizeLongestSide(input_img_size) # can be changed?\n",
    "\n",
    "    #set_image\n",
    "    input_image = transform.apply_image(image)\n",
    "    input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "    input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "\n",
    "    #set_torch_image\n",
    "    transformed_image = input_image_torch\n",
    "    input_size = tuple(transformed_image.shape[-2:])\n",
    "    transformed_image = model.preprocess(transformed_image)\n",
    "    features = model.image_encoder(transformed_image)\n",
    "\n",
    "\n",
    "    coords_torch, labels_torch, box_torch, mask_input_torch = None, None, None, None\n",
    "\n",
    "    # TO DELETE\n",
    "    t1 = np.array([70,235])\n",
    "    t2 = np.array([218,92])\n",
    "    t3 = np.array([154,360])\n",
    "    ts = np.array([t1,t2,t3])\n",
    "    # # background\n",
    "    b1 = np.array([259,257])\n",
    "    b2 = np.array([192,148])\n",
    "    b3 = np.array([220,435])\n",
    "    bs = np.array([b1,b2,b3])\n",
    "    ps = np.array([t1,t2,t3,b1,b2,b3])\n",
    "\n",
    "    labels = np.array([1,1,1,0,0,0])\n",
    "    \n",
    "\n",
    "\n",
    "    point_coords = transform.apply_coords(ps, org_shape[:2])\n",
    "    coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=device)\n",
    "    labels_torch = torch.as_tensor(labels, dtype=torch.int, device=device)\n",
    "    coords_torch, labels_torch = coords_torch[None, :, :], labels_torch[None, :]\n",
    "\n",
    "\n",
    "    points=(coords_torch, labels_torch)\n",
    "    sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
    "            points=points,\n",
    "            boxes=box_torch,\n",
    "            masks=mask_input_torch,\n",
    "        )\n",
    "\n",
    "    # Predict masks\n",
    "    low_res_masks, iou_predictions = model.mask_decoder(\n",
    "        image_embeddings = features,\n",
    "        image_pe= model.prompt_encoder.get_dense_pe(),\n",
    "        sparse_prompt_embeddings=sparse_embeddings,\n",
    "        dense_prompt_embeddings=dense_embeddings,\n",
    "        multimask_output= True,\n",
    "    )\n",
    "\n",
    "    # Upscale the masks to the original image resolution\n",
    "    masks = model.postprocess_masks(low_res_masks, input_size, org_shape[:2])\n",
    "\n",
    "\n",
    "    return masks, iou_predictions, low_res_masks\n",
    "    #return masks[0].detach().cpu().numpy(), iou_predictions[0].detach().cpu().numpy(), low_res_masks[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "(512, 512, 3)\n",
      "(512, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('data/Data/test/image/0.png')\n",
    "#image = Image.open('eye_enchanced.png')\n",
    "print(image.size)\n",
    "image = np.asarray(image)\n",
    "#image = image[0:254,0:254]\n",
    "#image = image[0:1024,0:1024]\n",
    "print(image.shape)\n",
    "\n",
    "\n",
    "\n",
    "image_tru = Image.open('data/Data/test/mask/0.png')\n",
    "print(image_tru.size)\n",
    "image_tru = np.asarray(image_tru)\n",
    "image_tru.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 512, 512])\n",
      "tensor([[0.0805, 0.1544]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masks, scores, low_masks = predict(mobile_sam_f, 512, image)\n",
    "print(masks.shape)\n",
    "print(scores)\n",
    "#results = format_results(masks, scores, logits, filter=0)\n",
    "#results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 2, 512, 512])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newma\\AppData\\Local\\Temp\\ipykernel_23168\\2027797337.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  image_tru = torch.tensor([image_tru]).long()\n"
     ]
    }
   ],
   "source": [
    "masks = masks.detach().cpu()\n",
    "print(type(masks))  \n",
    "print(masks.shape)\n",
    "image_tru = np.where(image_tru>0,1,0)\n",
    "image_tru = torch.tensor([image_tru]).long()\n",
    "print(type(image_tru))\n",
    "print(image_tru.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512])\n",
      "[0 1]\n",
      "torch.Size([1, 2, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "ones = len(np.where(image_tru>0)[0])\n",
    "zeros = len(np.where(image_tru==0)[0])\n",
    "\n",
    "print(image_tru.shape)  \n",
    "print(np.unique(image_tru))\n",
    "\n",
    "l0 = np.where(image_tru[0] < 0, 1, 0)\n",
    "l1 = np.where(image_tru[0] > 0, 1, 0)\n",
    "dummy_mask = np.zeros((1,2,512,512))\n",
    "dummy_mask[0][0] = l0\n",
    "dummy_mask[0][1] = l1\n",
    "dummy_mask = torch.tensor(dummy_mask).float()\n",
    "print(dummy_mask.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4905)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([0.126,1]).float())\n",
    "loss = criterion(dummy_mask, image_tru)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input identical True\n",
      "input_image_torch(after cast):  True\n",
      "input_image_torch (after permute)  True\n",
      "input_image after preproc:  True\n",
      "features identical True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.7350938 , 0.91377735, 0.97557175], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_f.set_image(image)\n",
    "FEAT_AUTO = predictor_f.features\n",
    "#masks, scores, logits = predictor_f.predict(None,None)#, multimask_output=True)\n",
    "\n",
    "# 70,235  218, 92     154,360 \n",
    "# 259,257    192, 148  220,435\n",
    "\n",
    "# doggo\n",
    "# p1 = np.array([940,730]) \n",
    "# p2 = np.array([1500,850]) \n",
    "# p3 = np.array([1100,400])\n",
    "# ps = np.array([p1,p2,p3])\n",
    "\n",
    "#0.png\n",
    "# target\n",
    "t1 = np.array([70,235])\n",
    "t2 = np.array([218,92])\n",
    "t3 = np.array([154,360])\n",
    "ts = np.array([t1,t2,t3])\n",
    "# background\n",
    "b1 = np.array([259,257])\n",
    "b2 = np.array([192,148])\n",
    "b3 = np.array([220,435])\n",
    "bs = np.array([b1,b2,b3])\n",
    "ps = np.array([t1,t2,t3,b1,b2,b3])\n",
    "\n",
    "labels = np.array([1,1,1,0,0,0])\n",
    "\n",
    "masks, scores, logits = predictor_f.predict(ps,point_labels= labels)\n",
    "# p,point_labels= np.array([1])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "q = put_mask(image,masks[0])\n",
    "\n",
    "q = put_marks(q, ps[3:], 10)\n",
    "q = put_marks(q, ps[0:3], 10, target=True)\n",
    "\n",
    "\n",
    "\n",
    "Image.fromarray(q).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os import getcwd\n",
    "from generate_target_ps import choose_bg_points, choose_target_points\n",
    "\n",
    "def load_data(path):\n",
    "    curr_dir = getcwd()\n",
    "    # images\n",
    "    imgs_path = curr_dir+'/data/Data/train/image/'\n",
    "    imgs_names = [f for f in listdir(imgs_path) if isfile(join(imgs_path, f))]\n",
    "    # masks\n",
    "    masks_path = curr_dir+'/data/Data/train/mask/'\n",
    "    msks_names = [f for f in listdir(masks_path) if isfile(join(masks_path, f))]\n",
    "\n",
    "    data = {'image': [], 'mask': [], 't_points': [], 'bg_points': []}\n",
    "\n",
    "\n",
    "\n",
    "    for img_name in imgs_names:\n",
    "        img = Image.open(imgs_path+img_name)\n",
    "        img = np.asarray(img)\n",
    "        data['image'].append(img)\n",
    "    \n",
    "    ts = 5\n",
    "    bgs = 5\n",
    "\n",
    "    for msk_name in msks_names:\n",
    "        msk = Image.open(masks_path+msk_name)\n",
    "        msk = np.asarray(msk)\n",
    "        t_points = choose_target_points(msk, ts, min_dist=50)\n",
    "        bg_points = choose_bg_points(msk, bgs, min_dist=50)\n",
    "        data['t_points'].append(t_points)\n",
    "        data['bg_points'].append(bg_points)\n",
    "        data['mask'].append(msk)\n",
    "    return data\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance applied:  45\n",
      "distance applied:  45\n",
      "distance applied:  45\n",
      "distance applied:  45\n"
     ]
    }
   ],
   "source": [
    "data = load_data('data/Data/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "input feature has wrong size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 59\u001b[0m         img_emb \u001b[38;5;241m=\u001b[39m \u001b[43msam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# get default embeddings\u001b[39;00m\n\u001b[0;32m     62\u001b[0m sparse_emb, dense_emb \u001b[38;5;241m=\u001b[39m sam\u001b[38;5;241m.\u001b[39mprompt_encoder(\n\u001b[0;32m     63\u001b[0m     points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     64\u001b[0m     boxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     65\u001b[0m     masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     66\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:721\u001b[0m, in \u001b[0;36mTinyViT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 721\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m#x = self.norm_head(x)\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;66;03m#x = self.head(x)\u001b[39;00m\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:712\u001b[0m, in \u001b[0;36mTinyViT.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_i, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[0;32m    711\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[1;32m--> 712\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;66;03m#print(i,x.shape)\u001b[39;00m\n\u001b[0;32m    714\u001b[0m B,_,C\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:537\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    535\u001b[0m         x \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(blk, x)\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m#print('block idx:',self.block_idx,'out_dim:',x.shape)\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\newma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\KU\\ATIA\\finetuneSAM\\models\\sam\\modeling\\tiny_vit_sam.py:372\u001b[0m, in \u001b[0;36mTinyViTBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    370\u001b[0m H, W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_resolution\n\u001b[0;32m    371\u001b[0m B, L, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m L \u001b[38;5;241m==\u001b[39m H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput feature has wrong size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m res_x \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m H \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;129;01mand\u001b[39;00m W \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size:\n",
      "\u001b[1;31mAssertionError\u001b[0m: input feature has wrong size"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sam = mobile_sam_f\n",
    "train_points = len(data['image'])\n",
    "checkpoints_path = 'checkpoints'\n",
    "\n",
    "\n",
    "if args.finetune_type == 'adapter':\n",
    "    for n, value in sam.named_parameters():\n",
    "        if \"Adapter\" not in n: # only update parameters in adapter\n",
    "            value.requires_grad = False\n",
    "    print('if update encoder:',args.if_update_encoder)\n",
    "    print('if image encoder adapter:',args.if_encoder_adapter)\n",
    "    print('if mask decoder adapter:',args.if_mask_decoder_adapter)\n",
    "    if args.if_encoder_adapter:\n",
    "        print('added adapter layers:',args.encoder_adapter_depths)\n",
    "    \n",
    "# elif args.finetune_type == 'vanilla' and args.if_update_encoder==False:   \n",
    "#     print('if update encoder:',args.if_update_encoder)\n",
    "#     for n, value in sam.image_encoder.named_parameters():\n",
    "#         value.requires_grad = False\n",
    "# elif args.finetune_type == 'lora':\n",
    "#     print('if update encoder:',args.if_update_encoder)\n",
    "#     print('if image encoder lora:',args.if_encoder_lora_layer)\n",
    "#     print('if mask decoder lora:',args.if_decoder_lora_layer)\n",
    "#     sam = LoRA_Sam(args,sam,r=4).sam\n",
    "# sam.to('cuda')\n",
    "b_lr = args.lr\n",
    "epochs = 80\n",
    "    \n",
    "optimizer = optim.AdamW(sam.parameters(), lr=b_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1, amsgrad=False)\n",
    "optimizer.zero_grad()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5) #learning rate decay\n",
    "criterion1 = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, to_onehot_y=True,reduction='mean')\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "iter_num = 0\n",
    "max_iterations = epochs * train_points\n",
    "writer = SummaryWriter(checkpoints_path + '/log')\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "val_largest_dsc = 0\n",
    "last_update_epoch = 0\n",
    "for epoch in pbar:\n",
    "    sam.train()\n",
    "    train_loss = 0\n",
    "    # batches here?\n",
    "    #for i,_ in enumerate(tqdm(data['image'])):\n",
    "    for i in range(1):\n",
    "        imgs = data['image'][i][0].cuda()\n",
    "        imgs = imgs.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "        imgs = imgs.float()\n",
    "        msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'][i])\n",
    "        msks = msks.cuda()\n",
    "\n",
    "\n",
    "\n",
    "        if args.if_update_encoder:\n",
    "            img_emb = sam.image_encoder(imgs)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                img_emb = sam.image_encoder(imgs)\n",
    "        \n",
    "        # get default embeddings\n",
    "        sparse_emb, dense_emb = sam.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "        pred, _ = sam.mask_decoder(\n",
    "                        image_embeddings=img_emb,\n",
    "                        image_pe=sam.prompt_encoder.get_dense_pe(), \n",
    "                        sparse_prompt_embeddings=sparse_emb,\n",
    "                        dense_prompt_embeddings=dense_emb, \n",
    "                        #multimask_output=True,\n",
    "                        multimask_output=False,\n",
    "                        )\n",
    "\n",
    "        #loss_dice = criterion1(pred,msks.float()) \n",
    "        loss_ce = criterion2(pred,torch.squeeze(msks.long(),1))\n",
    "        loss =  loss_ce\n",
    "        #loss =  loss_dice + loss_ce\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if args.if_warmup and iter_num < args.warmup_period:\n",
    "            lr_ = args.lr * ((iter_num + 1) / args.warmup_period)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_\n",
    "\n",
    "        else:\n",
    "            if args.if_warmup:\n",
    "                shift_iter = iter_num - args.warmup_period\n",
    "                assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n",
    "                lr_ = args.lr * (1.0 - shift_iter / max_iterations) ** 0.9  # learning rate adjustment depends on the max iterations\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_\n",
    "            else:\n",
    "                lr_ = args.lr\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        iter_num+=1\n",
    "        writer.add_scalar('info/lr', lr_, iter_num)\n",
    "        writer.add_scalar('info/total_loss', loss, iter_num)\n",
    "        writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n",
    "        #writer.add_scalar('info/loss_dice', loss_dice, iter_num)\n",
    "\n",
    "    train_loss /= (i+1)\n",
    "    pbar.set_description('Epoch num {}| train loss {} \\n'.format(epoch,train_loss))\n",
    "\n",
    "    # if epoch%2==0:\n",
    "    #     eval_loss=0\n",
    "    #     dsc = 0\n",
    "    #     sam.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         for i in tqdm(data['image']):\n",
    "    #             imgs = data['image'][i].cuda()\n",
    "    #             msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'][i])\n",
    "    #             msks = msks.cuda()\n",
    "\n",
    "    #             img_emb= sam.image_encoder(imgs)\n",
    "    #             sparse_emb, dense_emb = sam.prompt_encoder(\n",
    "    #                 points=None,\n",
    "    #                 boxes=None,\n",
    "    #                 masks=None,\n",
    "    #             )\n",
    "    #             pred, _ = sam.mask_decoder(\n",
    "    #                             image_embeddings=img_emb,\n",
    "    #                             image_pe=sam.prompt_encoder.get_dense_pe(), \n",
    "    #                             sparse_prompt_embeddings=sparse_emb,\n",
    "    #                             dense_prompt_embeddings=dense_emb, \n",
    "    #                             multimask_output=True,\n",
    "    #                             )\n",
    "    #             loss = criterion1(pred,msks.float()) + criterion2(pred,torch.squeeze(msks.long(),1))\n",
    "    #             eval_loss +=loss.item()\n",
    "    #             dsc_batch = dice_coeff_multi_class(pred.argmax(dim=1).cpu(), torch.squeeze(msks.long(),1).cpu().long(),args.num_cls)\n",
    "    #             dsc+=dsc_batch\n",
    "\n",
    "    #         eval_loss /= (i+1)\n",
    "    #         dsc /= (i+1)\n",
    "            \n",
    "    #         writer.add_scalar('eval/loss', eval_loss, epoch)\n",
    "    #         writer.add_scalar('eval/dice', dsc, epoch)\n",
    "            \n",
    "    #         print('Eval Epoch num {} | val loss {} | dsc {} \\n'.format(epoch,eval_loss,dsc))\n",
    "    #         if dsc>val_largest_dsc:\n",
    "    #             val_largest_dsc = dsc\n",
    "    #             last_update_epoch = epoch\n",
    "    #             print('largest DSC now: {}'.format(dsc))\n",
    "    #             torch.save(sam.state_dict(),checkpoints_path + '/checkpoint_best.pth')\n",
    "    #         elif (epoch-last_update_epoch)>20:\n",
    "    #             # the network haven't been updated for 20 epochs\n",
    "    #             print('Training finished###########')\n",
    "    #             break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3a3663b3d63a4c4cac421d8fd4746051": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ffd23cd526d49bbb04bb3fb3713d38c",
      "placeholder": "​",
      "style": "IPY_MODEL_e3bd26913d7d48ba8622d17d290ed9b9",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "4988b977dc674fde92fe781df557bd9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "762364a1ca11410894f2ba1104b3b81f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0caf5e9e72d4b029a7bb4d10baed9e8",
      "placeholder": "​",
      "style": "IPY_MODEL_937b51bfed124a47a46c4773998531f5",
      "value": " 466/466 [00:00&lt;00:00, 33.8kB/s]"
     }
    },
    "7ab0cc445a37426c89c5710c15db6227": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a3663b3d63a4c4cac421d8fd4746051",
       "IPY_MODEL_ab8c2f638a7e49bc95f5f3675cdfb737",
       "IPY_MODEL_762364a1ca11410894f2ba1104b3b81f"
      ],
      "layout": "IPY_MODEL_8aa7ec419c4a4c7db62e6fcc1df27498"
     }
    },
    "7ffd23cd526d49bbb04bb3fb3713d38c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8aa7ec419c4a4c7db62e6fcc1df27498": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "937b51bfed124a47a46c4773998531f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0caf5e9e72d4b029a7bb4d10baed9e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab8c2f638a7e49bc95f5f3675cdfb737": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df9e5b3a11c642c5b32b84a693751ac3",
      "max": 466,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4988b977dc674fde92fe781df557bd9a",
      "value": 466
     }
    },
    "df9e5b3a11c642c5b32b84a693751ac3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3bd26913d7d48ba8622d17d290ed9b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
